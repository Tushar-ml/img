{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_output_classifier.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Ecour4h077fWJYq1unoaBYwEhIX0S3DD","authorship_tag":"ABX9TyNNVSWn0uFexGbOvYY5HbsM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tadXj1ODv7Wh","executionInfo":{"status":"ok","timestamp":1610881642469,"user_tz":-330,"elapsed":1088,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"2a9a2bce-d64e-4027-fb5b-5c9adf064c1a"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Jan 17 11:07:22 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   56C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PY__-nJAv8gp","executionInfo":{"status":"ok","timestamp":1610882464679,"user_tz":-330,"elapsed":1046,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}}},"source":["# !pip install tez\r\n","# !pip install efficientnet_pytorch\r\n","# !pip uninstall albumentations\r\n","# !pip install albumentations"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"toc9H5NBB8Gq"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"CB6yHbhPvnUW"},"source":["import os\r\n","import albumentations\r\n","import  matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","\r\n","import tez\r\n","\r\n","from tez.datasets import ImageDataset\r\n","from tez.callbacks import EarlyStopping\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","\r\n","import torchvision\r\n","\r\n","from sklearn import metrics, model_selection\r\n","from efficientnet_pytorch import EfficientNet\r\n","from pathlib import Path\r\n","import argparse\r\n","import os\r\n","\r\n","import albumentations\r\n","import pandas as pd\r\n","import tez\r\n","import torch\r\n","import torch.nn as nn\r\n","from efficientnet_pytorch import EfficientNet\r\n","from sklearn import metrics, model_selection, preprocessing\r\n","from tez.callbacks import EarlyStopping, Callback\r\n","from tez.datasets import ImageDataset\r\n","from torch.nn import functional as F\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","from tqdm import tqdm\r\n","\r\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7jWlwUzvz-t"},"source":["# Dataset Class"]},{"cell_type":"code","metadata":{"id":"GRqCZJMfv29f"},"source":["# =====================================================================\r\n","# Dataset                                                        =\r\n","# =====================================================================\r\n","\r\n","\r\n","class ImageVecDataset(Dataset):\r\n","\r\n","    def __init__(self, IMAGE_PATH, DF_PATH):\r\n","        \"\"\"\r\n","        Args:\r\n","            csv_file (string): Path to the csv file with annotations.\r\n","            root_dir (string): Directory with all the images.\r\n","        \"\"\"\r\n","        self.image_dir = IMAGE_PATH\r\n","        self.df = pd.read_csv(DF_PATH)\r\n","        self.images = [str(i) + '.npy' for i in self.df.id.tolist()]\r\n","        self.labels = None\r\n","        if 'gender' in self.df.columns.tolist():\r\n","            self.genderLabels = self.df.gender.tolist()\r\n","            self.masterCategoryLabels = self.df.masterCategory.tolist()\r\n","            self.subCategoryLabels = self.df.subCategory.tolist()\r\n","            self.articleTypeLabels = self.df.articleType.tolist()\r\n","            self.baseColourLabels = self.df.baseColour.tolist()\r\n","            self.seasonLabels = self.df.season.tolist()\r\n","            self.usageLabels = self.df.usage.tolist()\r\n","            self.labels = True\r\n","        else:\r\n","            self.labels = None\r\n","\r\n","    def __len__(self):\r\n","        return len(self.images)\r\n","\r\n","    def __getitem__(self, idx):\r\n","      filename =self.images[idx]\r\n","      \r\n","      \r\n","      img = np.load(os.path.join(self.image_dir, filename))\r\n","      img = torch.from_numpy(img)\r\n","      if self.labels is not None:\r\n","          \r\n","          genderLabel = torch.tensor(self.genderLabels[idx])\r\n","          masterCategoryLabel = torch.tensor(self.masterCategoryLabels[idx])\r\n","          subCategoryLabel = torch.tensor(self.subCategoryLabels[idx])\r\n","          articleTypeLabel = torch.tensor(self.articleTypeLabels[idx])\r\n","          baseColourLabel = torch.tensor(self.baseColourLabels[idx])\r\n","          seasonLabel = torch.tensor(self.seasonLabels[idx])\r\n","          usageLabel = torch.tensor(self.usageLabels[idx])\r\n","\r\n","        #   return {'image': img, 'genderLabel': genderLabel, 'seasonLabel': seasonLabel}\r\n","\r\n","          return {'image': img, 'genderLabel': genderLabel, 'masterCategoryLabel': masterCategoryLabel, 'subCategoryLabel': subCategoryLabel, \r\n","                        'articleTypeLabel': articleTypeLabel, 'baseColourLabel': baseColourLabel, 'seasonLabel': seasonLabel, 'usageLabel': usageLabel\r\n","          }\r\n","      return {'image': img}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"esuZ-LlbwZh_"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"61Jy9is0wYh-"},"source":["# =====================================================================\r\n","# Model                                                        =\r\n","# =====================================================================\r\n","\r\n","\r\n","class FashionModel(tez.Model):\r\n","    def __init__(self, num_classes):\r\n","        super().__init__()\r\n","\r\n","        # self.effnet = EfficientNet.from_pretrained(\"efficientnet-b3\")\r\n","        in_features = 1536\r\n","        # Layer 1\r\n","        self.bn1 = nn.BatchNorm1d(num_features=in_features)\r\n","        self.dropout1 = nn.Dropout(0.25)\r\n","        self.linear1 = nn.Linear(in_features=in_features, out_features=512, bias=False)\r\n","        self.relu = nn.ReLU()\r\n","        # Layer 2\r\n","        self.bn2 = nn.BatchNorm1d(num_features=512)\r\n","        self.dropout2 = nn.Dropout(0.5)\r\n","\r\n","        self.gender = nn.Linear(512, num_classes['gender'])\r\n","        self.masterCategory = nn.Linear(512, num_classes['masterCategory'])\r\n","        self.subCategory = nn.Linear(512, num_classes['subCategory'])\r\n","        self.articleType = nn.Linear(512, num_classes['articleType'])\r\n","        self.baseColour = nn.Linear(512, num_classes['baseColour'])\r\n","        self.season = nn.Linear(512, num_classes['season'])\r\n","        self.usage = nn.Linear(512, num_classes['usage'])\r\n","       \r\n","        self.step_scheduler_after = \"epoch\"\r\n","\r\n","\r\n","    def monitor_metrics(self, outputs, targets):\r\n","        if targets is None:\r\n","            return {}\r\n","        accuracy = []\r\n","        for k,v in outputs.items():\r\n","            out = outputs[k]\r\n","            targ = targets[k]\r\n","            # print(out)\r\n","            out = torch.argmax(out, dim=1).cpu().detach().numpy()\r\n","            targ = targ.cpu().detach().numpy()\r\n","            accuracy.append(metrics.accuracy_score(targ, out))\r\n","        return {'accuracy': sum(accuracy)/len(accuracy)}\r\n","\r\n","    def fetch_optimizer(self):\r\n","        opt = torch.optim.Adam(self.parameters(), lr=3e-4)\r\n","        return opt\r\n","\r\n","    def fetch_scheduler(self):\r\n","        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\r\n","            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\r\n","        )\r\n","        return sch\r\n","\r\n","    def forward(self, image, genderLabel=None, masterCategoryLabel=None, subCategoryLabel=None, articleTypeLabel=None, baseColourLabel=None, seasonLabel=None, usageLabel=None):\r\n","        batch_size, _ = image.shape\r\n","\r\n","        x = self.linear1(self.dropout1(self.bn1(image)))\r\n","\r\n","        x = self.relu(x)\r\n","\r\n","        x = self.dropout2(self.bn2(x))\r\n","\r\n","        targets = {}\r\n","        if genderLabel is None:\r\n","            targets = None\r\n","        else:\r\n","            targets['gender'] = genderLabel\r\n","            targets['masterCategory'] = masterCategoryLabel\r\n","            targets['subCategory'] = subCategoryLabel\r\n","            targets['articleType'] = articleTypeLabel\r\n","            targets['baseColour'] = baseColourLabel\r\n","            targets['season'] = seasonLabel\r\n","            targets['usage'] = usageLabel\r\n","        outputs = {}\r\n","        outputs[\"gender\"] = self.gender(x)\r\n","        outputs[\"masterCategory\"] = self.masterCategory(x)\r\n","        outputs[\"subCategory\"] = self.subCategory(x)\r\n","        outputs[\"articleType\"] = self.articleType(x)\r\n","        outputs[\"baseColour\"] = self.baseColour(x)\r\n","        outputs[\"season\"] = self.season(x)\r\n","        outputs[\"usage\"] = self.usage(x)\r\n","        loss = []\r\n","        for k,v in outputs.items():\r\n","            loss.append(nn.CrossEntropyLoss()(outputs[k], targets[k]))\r\n","        loss = sum(loss)\r\n","        metrics = self.monitor_metrics(outputs, targets)\r\n","        return outputs, loss, metrics\r\n","    \r\n","    def extract_features(self, image):\r\n","        batch_size, _ = image.shape\r\n","\r\n","        features = self.linear1(self.dropout1(self.bn1(image)))\r\n","\r\n","        return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"acHw8zOiEF9Q"},"source":["\r\n","class SaveEfterEpoch(Callback):\r\n","    def __init__(self, model_path, model_name='epoch', save_interval=5):\r\n","        self.save_interval = save_interval\r\n","        self.model_path = model_path\r\n","        self.model_name = model_name\r\n","    \r\n","    def on_epoch_end(self, model):\r\n","        if model.current_epoch % self.save_interval == 0:\r\n","            model.save(os.path.join(self.model_path, f\"{self.model_name}_{model.current_epoch}.bin\"))\r\n","            print(f\"model saved at epoch: {model.current_epoch}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CX-JFVPZwdo2"},"source":["# Trainer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8GW19vOwc_o","executionInfo":{"status":"ok","timestamp":1610852987625,"user_tz":-330,"elapsed":942096,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"a538e422-af8f-4ebc-fe8b-187493b6cef1"},"source":["BASE_DIR = Path('/content/drive/MyDrive/upwork/img')\r\n","IMAGE_VECTOR_PATH = Path('/content/drive/MyDrive/upwork/img/data/fashion-dataset/image_vectors')\r\n","IMAGE_PATH = Path('/content/drive/MyDrive/upwork/img/data/fashion-dataset/images')\r\n","MODEL_PATH = BASE_DIR/'models'\r\n","\r\n","MODEL_NAME = 'fd_multi_output_classifier_v1'\r\n","TRAIN_BATCH_SIZE = 32\r\n","VALID_BATCH_SIZE = 32\r\n","EPOCHS = 30\r\n","\r\n","\r\n","image_path_list = list(Path(IMAGE_PATH).glob('*.jpg'))\r\n","print('Number of images: ', len(image_path_list))\r\n","dfx = pd.read_csv(BASE_DIR/'df_final.csv')\r\n","\r\n","train_d = ImageVecDataset(IMAGE_PATH=IMAGE_VECTOR_PATH, DF_PATH=BASE_DIR/'train.csv')\r\n","train_dl = DataLoader(train_d, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=8)\r\n","\r\n","val_d = ImageVecDataset(IMAGE_PATH=IMAGE_VECTOR_PATH, DF_PATH=BASE_DIR/'test.csv')\r\n","val_dl = DataLoader(val_d, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=8)\r\n","\r\n","\r\n","class_dict = dict([[i, dfx[i].nunique()] for i in dfx.columns.tolist()[1:]])\r\n","\r\n","model = FashionModel(num_classes=class_dict)\r\n","\r\n","es = EarlyStopping(\r\n","    monitor=\"valid_loss\",\r\n","    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\r\n","    patience=5,\r\n","    mode=\"min\",\r\n",")\r\n","\r\n","sfe = SaveEfterEpoch(save_interval=5, model_path=MODEL_PATH, model_name='fashion_model_epoch')\r\n","\r\n","model.fit(\r\n","    train_d,\r\n","    valid_dataset=val_d,\r\n","    train_bs=TRAIN_BATCH_SIZE,\r\n","    valid_bs=VALID_BATCH_SIZE,\r\n","    device='cuda',\r\n","    epochs=EPOCHS,\r\n","    callbacks=[es, sfe],\r\n","    fp16=False,\r\n",")\r\n","\r\n","model.save(os.path.join(MODEL_PATH, MODEL_NAME + \"_last.bin\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of images:  29163\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.91it/s, accuracy=0.728, loss=6.7, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.09it/s, accuracy=0.817, loss=4.07, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (inf --> 4.074792371929021). Saving model!\n","model saved at epoch: 0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.61it/s, accuracy=0.802, loss=4.25, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 24.95it/s, accuracy=0.833, loss=3.55, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (4.074792371929021 --> 3.5499677052155385). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.77it/s, accuracy=0.818, loss=3.8, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.22it/s, accuracy=0.84, loss=3.38, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.5499677052155385 --> 3.380411664425339). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.67it/s, accuracy=0.825, loss=3.57, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.47it/s, accuracy=0.843, loss=3.27, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.380411664425339 --> 3.2746643743462327). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:31<00:00, 23.31it/s, accuracy=0.832, loss=3.4, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.03it/s, accuracy=0.846, loss=3.2, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.2746643743462327 --> 3.199706006445279). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.72it/s, accuracy=0.839, loss=3.26, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 24.94it/s, accuracy=0.848, loss=3.14, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.199706006445279 --> 3.1442812424338324). Saving model!\n","model saved at epoch: 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.96it/s, accuracy=0.842, loss=3.16, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.37it/s, accuracy=0.852, loss=3.09, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.1442812424338324 --> 3.087424982977177). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.75it/s, accuracy=0.846, loss=3.09, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.20it/s, accuracy=0.852, loss=3.08, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.087424982977177 --> 3.0753210960830772). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.70it/s, accuracy=0.85, loss=3.01, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.42it/s, accuracy=0.854, loss=3.05, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.0753210960830772 --> 3.0493079825659484). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.87it/s, accuracy=0.851, loss=2.98, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 24.92it/s, accuracy=0.853, loss=3.05, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.0493079825659484 --> 3.048199286118397). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.83it/s, accuracy=0.839, loss=3.23, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.06it/s, accuracy=0.85, loss=3.14, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 1 out of 5\n","model saved at epoch: 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.68it/s, accuracy=0.84, loss=3.19, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.35it/s, accuracy=0.849, loss=3.12, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 2 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.81it/s, accuracy=0.844, loss=3.1, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.28it/s, accuracy=0.852, loss=3.08, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 3 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.32it/s, accuracy=0.846, loss=3.03, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 25.88it/s, accuracy=0.854, loss=3.05, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 4 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.22it/s, accuracy=0.851, loss=2.94, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.11it/s, accuracy=0.855, loss=3.02, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.048199286118397 --> 3.021265149775131). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.18it/s, accuracy=0.854, loss=2.86, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.07it/s, accuracy=0.855, loss=3.01, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.021265149775131 --> 3.0069400948055542). Saving model!\n","model saved at epoch: 15\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 24.10it/s, accuracy=0.858, loss=2.79, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.43it/s, accuracy=0.858, loss=2.98, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (3.0069400948055542 --> 2.9791411457799417). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.32it/s, accuracy=0.86, loss=2.73, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.11it/s, accuracy=0.858, loss=2.96, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (2.9791411457799417 --> 2.9580093174349535). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 24.12it/s, accuracy=0.864, loss=2.67, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 25.93it/s, accuracy=0.858, loss=2.95, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (2.9580093174349535 --> 2.9497835939101753). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.86it/s, accuracy=0.863, loss=2.66, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.00it/s, accuracy=0.86, loss=2.94, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation score improved (2.9497835939101753 --> 2.940000308811335). Saving model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.97it/s, accuracy=0.852, loss=2.91, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 24.96it/s, accuracy=0.855, loss=3.02, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 1 out of 5\n","model saved at epoch: 20\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 23.71it/s, accuracy=0.852, loss=2.89, stage=train]\n","100%|██████████| 181/181 [00:07<00:00, 25.55it/s, accuracy=0.855, loss=3.04, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 2 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:30<00:00, 24.04it/s, accuracy=0.855, loss=2.84, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.20it/s, accuracy=0.857, loss=3.01, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 3 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.19it/s, accuracy=0.856, loss=2.78, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.33it/s, accuracy=0.858, loss=2.99, stage=valid]\n","  0%|          | 0/724 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 4 out of 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 724/724 [00:29<00:00, 24.45it/s, accuracy=0.861, loss=2.73, stage=train]\n","100%|██████████| 181/181 [00:06<00:00, 26.47it/s, accuracy=0.859, loss=2.97, stage=valid]\n"],"name":"stderr"},{"output_type":"stream","text":["EarlyStopping counter: 5 out of 5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZtadT0DcXqYH"},"source":["# Fature Extractor"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYdVzCGzXMiu","executionInfo":{"status":"ok","timestamp":1610856078728,"user_tz":-330,"elapsed":5965,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"5b40268d-6492-43fa-dfd7-917a297d2717"},"source":["from tqdm import tqdm\r\n","\r\n","BASE_DIR = Path('/content/drive/MyDrive/upwork/img')\r\n","IMAGE_VECTOR_PATH = Path('/content/drive/MyDrive/upwork/img/data/fashion-dataset/image_vectors')\r\n","IMAGE_PATH = Path('/content/drive/MyDrive/upwork/img/data/fashion-dataset/images')\r\n","MODEL_PATH = BASE_DIR/'models'\r\n","\r\n","MODEL_NAME = 'fd_multi_output_classifier_v1'\r\n","TRAIN_BATCH_SIZE = 32\r\n","VALID_BATCH_SIZE = 32\r\n","EPOCHS = 30\r\n","\r\n","\r\n","image_path_list = list(Path(IMAGE_PATH).glob('*.jpg'))\r\n","print('Number of images: ', len(image_path_list))\r\n","dfx = pd.read_csv(BASE_DIR/'df_final.csv')\r\n","\r\n","\r\n","class_dict = dict([[i, dfx[i].nunique()] for i in dfx.columns.tolist()[1:]])\r\n","\r\n","model = FashionModel(num_classes=class_dict)\r\n","model.eval()\r\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of images:  29163\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["FashionModel(\n","  (bn1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout1): Dropout(p=0.25, inplace=False)\n","  (linear1): Linear(in_features=1536, out_features=512, bias=False)\n","  (relu): ReLU()\n","  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout2): Dropout(p=0.5, inplace=False)\n","  (gender): Linear(in_features=512, out_features=5, bias=True)\n","  (masterCategory): Linear(in_features=512, out_features=5, bias=True)\n","  (subCategory): Linear(in_features=512, out_features=28, bias=True)\n","  (articleType): Linear(in_features=512, out_features=75, bias=True)\n","  (baseColour): Linear(in_features=512, out_features=44, bias=True)\n","  (season): Linear(in_features=512, out_features=4, bias=True)\n","  (usage): Linear(in_features=512, out_features=7, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"g9XgcQWDeYLI"},"source":["# =====================================================================\r\n","# Dataset                                                        =\r\n","# =====================================================================\r\n","\r\n","\r\n","class ImageVecExtractDataset(Dataset):\r\n","\r\n","    def __init__(self, IMAGE_PATH, DF_PATH):\r\n","        \"\"\"\r\n","        Args:\r\n","            csv_file (string): Path to the csv file with annotations.\r\n","            root_dir (string): Directory with all the images.\r\n","        \"\"\"\r\n","        self.image_dir = IMAGE_PATH\r\n","        self.df = pd.read_csv(DF_PATH)\r\n","        self.images = [str(i) + '.npy' for i in self.df.id.tolist()]\r\n","\r\n","    def __len__(self):\r\n","        return len(self.images)\r\n","\r\n","    def __getitem__(self, idx):\r\n","      filename =self.images[idx]\r\n","      \r\n","      \r\n","      img = np.load(os.path.join(self.image_dir, filename))\r\n","      img = torch.from_numpy(img)\r\n","      \r\n","      return {'image': img, 'filename': filename}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyEID3iGXVKq","executionInfo":{"status":"ok","timestamp":1610856237618,"user_tz":-330,"elapsed":146522,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"e14cd6dc-3143-4fdc-fdc3-784dd61777f7"},"source":["image_path_list = list(Path(IMAGE_VECTOR_PATH).glob('*.npy'))\r\n","print('Number of images: ', len(image_path_list))\r\n","OUTPUT_PATH = '/content/drive/MyDrive/upwork/img/data/fashion-dataset/image_vector_512'\r\n","\r\n","f_dataset = ImageVecExtractDataset(IMAGE_PATH=IMAGE_VECTOR_PATH, DF_PATH=BASE_DIR/'df_final.csv')\r\n","f_dl = DataLoader(f_dataset, batch_size=32, shuffle=False, num_workers=8)\r\n","\r\n","model.load(os.path.join(os.path.join(MODEL_PATH, MODEL_NAME + \".bin\")))\r\n","\r\n","for i, data in tqdm(enumerate(f_dl), total=len(f_dl)):\r\n","  # print(data)\r\n","  image_vector = model.extract_features(data['image'].cuda()).cpu().detach().numpy()\r\n","  for i in range(len(data['filename'])):\r\n","    f_name = data['filename'][i]\r\n","    np.save(os.path.join(OUTPUT_PATH, f_name), image_vector[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of images:  29162\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 905/905 [02:24<00:00,  6.25it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jOoBNoSYdtq7"},"source":["# image_path_list = list(Path(OUTPUT_PATH).glob('*.npy'))\r\n","# print('Number of images: ', len(image_path_list))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xuz-rlqchLjP"},"source":["# np.load(image_path_list[0]).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPS5nfwe_TfO"},"source":["# Pipeline"]},{"cell_type":"code","metadata":{"id":"o9Va66ke_4bR","executionInfo":{"status":"ok","timestamp":1610882477093,"user_tz":-330,"elapsed":1316,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}}},"source":["import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtynGDmYAaJ4","executionInfo":{"status":"ok","timestamp":1610882477094,"user_tz":-330,"elapsed":1049,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}}},"source":["# !rm -rf '/content/drive/MyDrive/upwork/img/data/fashion-dataset/image_vector_512'\r\n","# !rm -rf '/content/drive/MyDrive/upwork/img/data/fashion-dataset/image_vectors'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YyH8SmJBEJj","executionInfo":{"status":"ok","timestamp":1610882477094,"user_tz":-330,"elapsed":874,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}}},"source":["root = '/content/drive/MyDrive/upwork/img'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6Z144AfBGvD","executionInfo":{"status":"ok","timestamp":1610882479241,"user_tz":-330,"elapsed":1262,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"58c87fab-e3c3-48d8-f5c9-1297f47b0e3b"},"source":["import os, sys\r\n","print( os.getcwd())\r\n","os.chdir(root)\r\n","print( os.getcwd())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/.shortcut-targets-by-id/120/img\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AkMm4UEe_98G","executionInfo":{"status":"ok","timestamp":1610882482892,"user_tz":-330,"elapsed":1060,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"877aaa03-d491-48c1-fa93-16d685679763"},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["data\t\t\t\t\t model_transfer.py\n","df_final.csv\t\t\t\t multi_output_classifier.ipynb\n","eda.ipynb\t\t\t\t styles.csv\n","fashion_dataset_feature_extractor.ipynb  test.csv\n","kaggle.json\t\t\t\t test_dataset\n","model1_new\t\t\t\t train.csv\n","models\t\t\t\t\t transfer.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iyiIbvxRAYqI"},"source":["# ! pwd\r\n","\r\n","# ### UNbuffer\r\n","# ! nohup  python3 model_transfer.py  &"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YWAyZSJhOQA"},"source":["# !python '/content/drive/MyDrive/upwork/img/model_transfer.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9yQm3C6Bp0J","executionInfo":{"status":"ok","timestamp":1610888158846,"user_tz":-330,"elapsed":5632507,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"9e4ddc1c-6093-40a4-c8cf-7df88adacfb9"},"source":["!python3 model_transfer.py"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['1', '2', '3']\n","Starting Stage 1 -------------------------\n","Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n","100% 47.1M/47.1M [00:00<00:00, 63.6MB/s]\n","Loaded pretrained weights for efficientnet-b3\n","100% 3619/3619 [1:12:02<00:00,  1.19s/it]\n","Stage 1 Finished -------------------------\n","Starting Stage 2 -------------------------\n","100% 2895/2895 [07:29<00:00,  6.44it/s, accuracy=0.728, loss=6.26, stage=train]\n","100% 724/724 [01:54<00:00,  6.34it/s, accuracy=0.814, loss=3.98, stage=valid]\n","Validation score improved (inf --> 3.981799150369444). Saving model!\n","model saved at epoch: 0\n","100% 2895/2895 [00:55<00:00, 52.30it/s, accuracy=0.78, loss=4.6, stage=train]\n","100% 724/724 [00:09<00:00, 74.08it/s, accuracy=0.823, loss=3.71, stage=valid]\n","Validation score improved (3.981799150369444 --> 3.7102390160547434). Saving model!\n","100% 2895/2895 [00:55<00:00, 51.99it/s, accuracy=0.792, loss=4.28, stage=train]\n","100% 724/724 [00:09<00:00, 77.20it/s, accuracy=0.83, loss=3.56, stage=valid]\n","Validation score improved (3.7102390160547434 --> 3.561649789467701). Saving model!\n","100% 2895/2895 [00:54<00:00, 52.96it/s, accuracy=0.801, loss=4.07, stage=train]\n","100% 724/724 [00:09<00:00, 75.43it/s, accuracy=0.832, loss=3.49, stage=valid]\n","Validation score improved (3.561649789467701 --> 3.488707827435014). Saving model!\n","100% 2895/2895 [00:54<00:00, 53.41it/s, accuracy=0.808, loss=3.94, stage=train]\n","100% 724/724 [00:09<00:00, 73.98it/s, accuracy=0.837, loss=3.39, stage=valid]\n","Validation score improved (3.488707827435014 --> 3.3947126727077843). Saving model!\n","100% 2895/2895 [00:54<00:00, 52.79it/s, accuracy=0.813, loss=3.79, stage=train]\n","100% 724/724 [00:10<00:00, 71.66it/s, accuracy=0.839, loss=3.36, stage=valid]\n","Validation score improved (3.3947126727077843 --> 3.362997220530694). Saving model!\n","model saved at epoch: 5\n","100% 2895/2895 [00:55<00:00, 51.93it/s, accuracy=0.817, loss=3.68, stage=train]\n","100% 724/724 [00:09<00:00, 72.54it/s, accuracy=0.842, loss=3.3, stage=valid]\n","Validation score improved (3.362997220530694 --> 3.3026668694467176). Saving model!\n","100% 2895/2895 [00:56<00:00, 51.20it/s, accuracy=0.822, loss=3.58, stage=train]\n","100% 724/724 [00:09<00:00, 73.80it/s, accuracy=0.844, loss=3.27, stage=valid]\n","Validation score improved (3.3026668694467176 --> 3.273058147884864). Saving model!\n","100% 2895/2895 [00:54<00:00, 52.78it/s, accuracy=0.824, loss=3.53, stage=train]\n","100% 724/724 [00:09<00:00, 74.18it/s, accuracy=0.846, loss=3.24, stage=valid]\n","Validation score improved (3.273058147884864 --> 3.2435119049983787). Saving model!\n","100% 2895/2895 [00:54<00:00, 53.04it/s, accuracy=0.825, loss=3.5, stage=train]\n","100% 724/724 [00:09<00:00, 73.83it/s, accuracy=0.846, loss=3.24, stage=valid]\n","Validation score improved (3.2435119049983787 --> 3.237829424729005). Saving model!\n","Stage 2 Finished -------------------------\n","Starting Stage 3 -------------------------\n","100% 3619/3619 [02:18<00:00, 26.20it/s]\n","Stage 3 Finished -------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gh90K3MRBrDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610888392244,"user_tz":-330,"elapsed":2058,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"6f0ffc07-c1cc-4c11-b1ef-fa46cd078e5d"},"source":["import torch \r\n","print(torch.__version__)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["1.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hv_fiVqVbiXw","executionInfo":{"status":"ok","timestamp":1610891122347,"user_tz":-330,"elapsed":2636963,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"a135057b-d8d6-4e0a-f54f-3088d2e88e13"},"source":["!python3 extract_vectors_big_model.py"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n","100% 1810/1810 [43:48<00:00,  1.45s/it]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MVpbK4HrbmSJ"},"source":[""],"execution_count":null,"outputs":[]}]}