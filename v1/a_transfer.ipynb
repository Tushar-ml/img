{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer.ipynb","provenance":[{"file_id":"https://github.com/openai/clip/blob/master/Interacting_with_CLIP.ipynb","timestamp":1610602560520}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNTbQjuLV8iJ","executionInfo":{"status":"ok","timestamp":1611918684793,"user_tz":-330,"elapsed":1129,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"a2fe0daf-96d3-4f32-d1a5-c4b93ee72ff4"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Jan 29 11:11:24 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"53N4k0pj_9qL"},"source":["# Preparation for Colab\n","\n","Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will print the CUDA version of the runtime if it has a GPU, and install PyTorch 1.7.1."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BpdJkdBssk9","executionInfo":{"status":"ok","timestamp":1611918690804,"user_tz":-330,"elapsed":1430,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"d74c8cae-bc89-4473-b6f2-74212d99a8a8"},"source":["import subprocess\n","\n","CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n","print(\"CUDA version:\", CUDA_version)\n","\n","if CUDA_version == \"10.0\":\n","    torch_version_suffix = \"+cu100\"\n","elif CUDA_version == \"10.1\":\n","    torch_version_suffix = \"+cu101\"\n","elif CUDA_version == \"10.2\":\n","    torch_version_suffix = \"\"\n","else:\n","    torch_version_suffix = \"+cu110\"\n","\n","# torch_version_suffix = '+cpu'   "],"execution_count":null,"outputs":[{"output_type":"stream","text":["CUDA version: 10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBVr18E5tse8","executionInfo":{"status":"ok","timestamp":1611918845998,"user_tz":-330,"elapsed":152661,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"2ad1c0ad-ff9f-4560-cd06-4684b648f869"},"source":["! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.7.1+cu101\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n","\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n","\u001b[?25hCollecting torchvision==0.8.2+cu101\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 244kB/s \n","\u001b[?25hCollecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=8e22735b99837255b93f1f0641694c69373e013e9ca9aeb8f9e13c8c64f5b1b3\n","  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n","Successfully built ftfy\n","Installing collected packages: torch, torchvision, ftfy\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: torchvision 0.8.1+cu101\n","    Uninstalling torchvision-0.8.1+cu101:\n","      Successfully uninstalled torchvision-0.8.1+cu101\n","Successfully installed ftfy-5.8 torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1hkDT38hSaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611911233516,"user_tz":-330,"elapsed":4600,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"e2715381-067c-436c-9773-7de39e66c183"},"source":["import numpy as np\n","import torch\n","\n","print(\"Torch version:\", torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Torch version: 1.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iuL8NJrUaRYv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLnRO-jLaTGq","executionInfo":{"status":"ok","timestamp":1610623869308,"user_tz":-540,"elapsed":24363,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"5e585038-ec73-4c35-ace8-9f40646e7be8"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EgeIE5QpaRbx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611826662164,"user_tz":-330,"elapsed":48330,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"9fdeabe8-175a-4eab-f777-7bbae62a59fb"},"source":["### Check if Shared folder is available :\r\n","\"\"\"\r\n","You need to create shorcut of this SHARED folder\r\n","\r\n","https://drive.google.com/drive/u/0/folders/1nS-yYgKNmkrelX9Z82BvjrGClI-Ar4Hp\r\n","\r\n","\r\n","IN your Google Drive\r\n","\r\n","    /MyDrive/Colab Notebooks/shared_session/\r\n","\r\n","\r\n","! ls \"/content/drive/MyDrive/Colab Notebooks/shared_session/\"\r\n","\r\n","\r\n","\"\"\"\r\n","\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qdv-XlIGaRf9","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"error","timestamp":1611911252040,"user_tz":-330,"elapsed":1104,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"11ccbe9c-08ca-439e-c563-744956d40c6f"},"source":["##### Go to root folder\r\n","root =   \"/content/drive/My Drive/Colab Notebooks/shared_session/img/\"\r\n","\r\n","import os, sys\r\n","print(os.getcwd())\r\n","os.chdir(root)\r\n","print( os.getcwd())\r\n","\r\n","! pwd && ls .\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6158572e5466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/shared_session/img/'"]}]},{"cell_type":"code","metadata":{"id":"N5-48_YjaRi6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFGfzm2Qpx7Z"},"source":["# Kaggle"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67UcfoRppxJJ","executionInfo":{"status":"ok","timestamp":1610623912280,"user_tz":-540,"elapsed":883,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"6a5583b6-6590-41ff-c83a-1506fb4802c3"},"source":["\"\"\"\r\n","https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a\r\n","\r\n","import os\r\n","os.environ['KAGGLE_CONFIG_DIR'] = root\r\n","# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\r\n","\r\n","\r\n","\r\n","\"\"\"\r\n","\r\n","import os\r\n","os.environ['KAGGLE_CONFIG_DIR'] = root\r\n","\r\n","print(  os.environ['KAGGLE_CONFIG_DIR'] )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/shared_session/img/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L59OTfIopxMA"},"source":["\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEy2UrfGpxOn","executionInfo":{"status":"ok","timestamp":1610624526118,"user_tz":-540,"elapsed":610795,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"e57ee908-67f4-449b-d865-5ff03e0c9f3b"},"source":["\r\n","\r\n","! kaggle datasets download -d paramaggarwal/fashion-product-images-dataset"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading fashion-product-images-dataset.zip to /content/drive/My Drive/Colab Notebooks/shared_session/img\n","100% 23.1G/23.1G [10:08<00:00, 53.3MB/s]\n","100% 23.1G/23.1G [10:08<00:00, 40.7MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2CKdacmpxTV","executionInfo":{"status":"ok","timestamp":1610624691190,"user_tz":-540,"elapsed":747,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"88f0e6b1-1558-46ed-fdf7-edbb35bcb766"},"source":["! ls -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 24190663\n","drwx------ 2 root root        4096 Jan 14 11:11 data\n","-rw------- 1 root root 24771215740 Jan 14 11:41 fashion-product-images-dataset.zip\n","-rw------- 1 root root          63 Jan 14 11:27 kaggle.json\n","-rw------- 1 root root       13839 Jan 14 11:43 transfer.ipynb\n","drwx------ 2 root root        4096 Jan 14 11:11 zperso\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0BRJJz4tKNh","executionInfo":{"status":"ok","timestamp":1610625149496,"user_tz":-540,"elapsed":717,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"49505e24-cab4-4e30-b72b-ae512e8b1daf"},"source":["! pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/shared_session/img\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ybP8zb0ipxUe"},"source":["! mv  fashion-product-images-dataset.zip  data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSNHhtyXtuHu","executionInfo":{"status":"ok","timestamp":1610626026627,"user_tz":-540,"elapsed":535856,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"0c3969a9-c60e-4c24-90b9-15e388f0fe2f"},"source":["! cd data &&  nohup unzip fashion-product-images-dataset.zip &\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["nohup: appending output to 'nohup.out'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eFxgLV5HAEEw"},"source":["# Downloading the model\n","TorchScript modules."]},{"cell_type":"code","metadata":{"id":"uLFS29hnhlY4","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1611252646687,"user_tz":-330,"elapsed":146746,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"0e8fe042-f1df-47c5-f0a1-3c5fe5808491"},"source":["\r\n","##### Go to root folder\r\n","root =   \"/content/drive/My Drive/Colab Notebooks/shared_session/img/\"\r\n","\r\n","import os, sys\r\n","print( os.getcwd())\r\n","os.chdir(root)\r\n","print( os.getcwd())\r\n","\r\n","! pwd && ls .\r\n","\r\n","\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-10515a96773f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/shared_session/img/'"]}]},{"cell_type":"code","metadata":{"id":"cboKZocQlSYX"},"source":["\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBRVTY9lbGm8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21slhZGCqANb"},"source":["# Image Preprocessing\n","\n","We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n","\n"]},{"cell_type":"code","metadata":{"id":"d6cpiIFHp9N6"},"source":["from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n","from PIL import Image\n","\n","preprocess = Compose([\n","    Resize(input_resolution, interpolation=Image.BICUBIC),\n","    CenterCrop(input_resolution),\n","    ToTensor()\n","])\n","\n","image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n","image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMc1AXzBlhzm"},"source":["import os\n","import skimage\n","import IPython.display\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","\n","from collections import OrderedDict\n","import torch\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvnxfiABmdQn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0h8uwIArnT4c"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Du5P8U79nT6_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JLxKZtgnT-J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHdRefHBnUAz"},"source":["\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GTg1hw1pLQ-L"},"source":["# Training Pipeline"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37_9NYBIKJdZ","executionInfo":{"status":"ok","timestamp":1611919166468,"user_tz":-330,"elapsed":20420,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"8fad0cbe-3102-42ca-a4a4-8f4bcbcbafc4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"al1M_-p5nUDh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611919182935,"user_tz":-330,"elapsed":20822,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"7ea78681-2571-4b08-f37a-08ce38a7152e"},"source":["!pip install tez\r\n","!pip install efficientnet_pytorch\r\n","!pip uninstall albumentations\r\n","!pip install albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tez\n","  Downloading https://files.pythonhosted.org/packages/56/37/9b99ae05da3fa2b05a4cbb0cf78c50dfdcf805b55f29029c3cb6c5a0fee1/tez-0.1.2-py3-none-any.whl\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tez) (1.7.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (3.7.4.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (0.8)\n","Installing collected packages: tez\n","Successfully installed tez-0.1.2\n","Collecting efficientnet_pytorch\n","  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.7.1+cu101)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16032 sha256=b762b442ba6c90b0605c5700676f2ce1e28d9c9188def8a6b74592e3dc69e143\n","  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.0\n","Uninstalling albumentations-0.1.12:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/albumentations-0.1.12.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/albumentations/*\n","Proceed (y/n)? y\n","  Successfully uninstalled albumentations-0.1.12\n","Collecting albumentations\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.5)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 8.0MB/s \n","\u001b[?25hCollecting opencv-python-headless>=4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n","\u001b[K     |████████████████████████████████| 37.6MB 85kB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n","Installing collected packages: imgaug, opencv-python-headless, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZxexkyDMmdTW"},"source":["# import os,sys\r\n","# !pip install tez -y\r\n","# !pip install efficientnet_pytorch\r\n","# !pip uninstall albumentations\r\n","# !pip install albumentations\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8s8XtLRcLZp5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611919187099,"user_tz":-330,"elapsed":1069,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"4e52ba66-2884-46ea-d06f-459f66fb7b6d"},"source":["root = '/content/drive/MyDrive/upwork/img'\r\n","# root =   \"/content/drive/My Drive/Colab Notebooks/shared_session/img/\"\r\n","\r\n","\r\n","import os, sys\r\n","print( os.getcwd())\r\n","os.chdir(root)\r\n","print( os.getcwd())\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/.shortcut-targets-by-id/1nS-yYgKNmkrelX9Z82BvjrGClI-Ar4Hp/img\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KG3ARmwdLa-D"},"source":["\"\"\"\r\n","\r\n","! nohup  python3 model_transfer.py  &\r\n","\r\n","!python3 model_transfer.py\r\n","\r\n","!python3 extract_vectors_big_model.py\r\n","\r\n","\r\n","\"\"\"\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NItceq8bLeT6","colab":{"base_uri":"https://localhost:8080/","height":164},"executionInfo":{"status":"error","timestamp":1611919376195,"user_tz":-330,"elapsed":121081,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"f0e8276e-0f8a-49c5-9055-ffcf3fedd1e4"},"source":["print(len(os.listdir('/content/drive/MyDrive/upwork/img/dataset/images')))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-b523d793f0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/upwork/img/dataset/images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/upwork/img/dataset/images'"]}]},{"cell_type":"code","metadata":{"id":"e4uX1u7-Lfw5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rCy9OCjakfe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611911385859,"user_tz":-330,"elapsed":3316,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"bfc6f243-6f41-4a5c-a812-9585c1b43754"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" data\t\t\t\t\t   model_transfer_v2.py\n"," dataset\t\t\t\t   model_transfer_v3.py\n"," df_final.csv\t\t\t\t   multi_output_classifier.ipynb\n"," df_final_pred.csv\t\t\t   nohup.out\n"," df_final_with_original_class.csv\t   pred_df.csv\n"," extract_vectors_big_model.py\t\t   styles.csv\n"," fashion-dataset\t\t\t   test.csv\n"," fashion_dataset_feature_extractor.ipynb   test_pred.csv\n"," kaggle.json\t\t\t\t  'test_pred.csv--batch_size=32'\n"," model1_new\t\t\t\t   train.csv\n"," models\t\t\t\t\t   transfer.ipynb\n"," model_transfer.py\t\t\t   val.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6AajRS11JMch"},"source":["# !unzip './data/fashion-product-images-dataset.zip' -d './data/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"4tJIfp3mebXD","executionInfo":{"status":"error","timestamp":1611851026466,"user_tz":-330,"elapsed":4763928,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"4737b234-7807-438d-c233-16085ccd47b8"},"source":["import time\r\n","while True:\r\n","  time.sleep(10)\r\n","  pass"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-395286a2deba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ol-JTfqOziu","executionInfo":{"status":"ok","timestamp":1611913433991,"user_tz":-330,"elapsed":1660,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"9a4559bd-8da7-495e-b67a-82e5ce06ba34"},"source":["len(os.listdir('./dataset/images'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["44441"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"wxegRlXquBxj"},"source":["## Stage 1 : DF Preprocess"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-sosrVGCchg","executionInfo":{"status":"ok","timestamp":1611913467631,"user_tz":-330,"elapsed":11797,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"c31f18ad-479b-49d1-ba3a-7011f2fa8d4e"},"source":["!python model_transfer_v3.py --stage='df_preprocess' --base_path='./' --image_path='./dataset/images' \\\r\n","                            --base_df='styles.csv' --root_df='df_final.csv' --train_df='train.csv' --val_df='val.csv'\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["df_preprocess\n","Total Number of Images:  44441\n","b'Skipping line 6044: expected 10 fields, saw 11\\nSkipping line 6569: expected 10 fields, saw 11\\nSkipping line 7399: expected 10 fields, saw 11\\nSkipping line 7939: expected 10 fields, saw 11\\nSkipping line 9026: expected 10 fields, saw 11\\nSkipping line 10264: expected 10 fields, saw 11\\nSkipping line 10427: expected 10 fields, saw 11\\nSkipping line 10905: expected 10 fields, saw 11\\nSkipping line 11373: expected 10 fields, saw 11\\nSkipping line 11945: expected 10 fields, saw 11\\nSkipping line 14112: expected 10 fields, saw 11\\nSkipping line 14532: expected 10 fields, saw 11\\nSkipping line 15076: expected 10 fields, saw 12\\nSkipping line 29906: expected 10 fields, saw 11\\nSkipping line 31625: expected 10 fields, saw 11\\nSkipping line 33020: expected 10 fields, saw 11\\nSkipping line 35748: expected 10 fields, saw 11\\nSkipping line 35962: expected 10 fields, saw 11\\nSkipping line 37770: expected 10 fields, saw 11\\nSkipping line 38105: expected 10 fields, saw 11\\nSkipping line 38275: expected 10 fields, saw 11\\nSkipping line 38404: expected 10 fields, saw 12\\n'\n","Dataset Size:  (44424, 10)\n","Total 5 images didn't found\n","Final Dataset Size:  (43905, 8)\n","Stage: df_preprocess Finished ------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3RUH9JUsCjTM"},"source":["## Stage 2 : Extract Efficient"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMmE0QFQy8hF","executionInfo":{"status":"ok","timestamp":1611834064460,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"1f151b5f-d227-44f1-b198-543c74d06005"},"source":["!python -V"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BvfkOj1CN7Dc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611913478023,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"bf2c38de-b686-421a-a9f3-c0fcf8547703"},"source":["# !python model_transfer_v2.py --stage='extract_efficienet' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#                              --output_vector_path='./data/fashion-dataset/efficienet_vectors'\r\n","\r\n","\r\n","\r\n","# !python model_transfer_v3.py --stage='extract_efficienet' --base_path='./' --image_path='./images' \\\r\n","#                             --output_vector_path='./data/fashion-dataset/efficientnet_vectors' \\\r\n","#                             --root_df='df_final.csv' --test_df='df_final.csv' --batch_size=32\r\n","\r\n","!nohup python3 model_transfer_v3.py --stage='extract_efficienet' --base_path='./' --image_path='./dataset/images' \\\r\n","                            --output_vector_path='./dataset/efficientnet_vectors' \\\r\n","                            --root_df='df_final.csv' --test_df='df_final.csv' --batch_size=16 &"],"execution_count":null,"outputs":[{"output_type":"stream","text":["nohup: appending output to 'nohup.out'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8slYUcqkel0q","executionInfo":{"status":"ok","timestamp":1611913536369,"user_tz":-330,"elapsed":1588,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"774a2cc6-332d-4bfa-f94f-15b1cff65fe2"},"source":["len(os.listdir('/content/drive/MyDrive/upwork/img/dataset/efficientnet_vectors'))\r\n","# len(os.listdir('/content/drive/MyDrive/upwork/img/data/fashion-dataset/efficientnet_vectors'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"V01TVYVIuLYY"},"source":["## Stage 2 : Extract Big Model1\r\n"]},{"cell_type":"code","metadata":{"id":"QxGBJDGoN7pT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611837187616,"user_tz":-330,"elapsed":1376,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"756b7d9c-9a72-44cf-bcc4-6e5365a30e54"},"source":["# !python model_transfer_v2.py --stage='extract_bigmodel1' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#                        --output_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='model'\r\n","\r\n","\r\n","# ! python model_transfer_v2.py --stage='extract_bigmodel1' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#                       --output_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='model'\r\n","\r\n","\r\n","# !python model_transfer_v3.py --stage='extract_bigmodel1' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#                             --output_vector_path='./fashion-dataset/big_model1_vectors' --model_name='model' \\\r\n","#                             --root_df='df_final.csv' --test_df='df_final.csv' --batch_size=32\r\n","\r\n","!nohup python3 model_transfer_v3.py --stage='extract_bigmodel1' --base_path='./' --image_path='./images' \\\r\n","                            --output_vector_path='./fashion-dataset/big_model1_vectors' --model_name='model' \\\r\n","                            --root_df='df_final.csv' --test_df='df_final.csv' --batch_size=32 &\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["nohup: appending output to 'nohup.out'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SVgikGO0uRi5"},"source":["## Stage 3 : train from big model1 vectors"]},{"cell_type":"code","metadata":{"id":"9OvPziNuN7sM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611841529034,"user_tz":-330,"elapsed":8701,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"e6d0474a-b921-4a3d-cb0c-a633f82876a6"},"source":["# # !python model_transfer_v2.py --stage='train_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","# #         --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='fashion_big_model1' \\\r\n","# #         --input_vector_size=512 --intermediate_vector_size=128\r\n","\r\n","\r\n","!python model_transfer_v3.py --stage='train_small' --base_path='./' --image_path='./images' \\\r\n","        --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='fashion_big_model1' \\\r\n","        --input_vector_size=512 --intermediate_vector_size=128 --root_df='df_final.csv' --train_df='train.csv' --val_df='val.csv' --batch_size=32 --epochs=20 --save_interval=5\r\n","\r\n","\r\n","# !nohup python3 model_transfer_v3.py --stage='train_small' --base_path='./' --image_path='./images' \\\r\n","#         --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='fashion_big_model1' \\\r\n","#         --input_vector_size=512 --intermediate_vector_size=128 --root_df='df_final.csv' --train_df='train.csv' \\\r\n","#         --val_df='val.csv' --batch_size=32 --epochs=20 --save_interval=5 &\r\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_small\n","Total Number of Images:  44441\n","Starting  Train Small model -------------------------\n","  0% 0/1098 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"model_transfer_v3.py\", line 532, in <module>\n","    fp16          = args.fp16,\n","  File \"/usr/local/lib/python3.6/dist-packages/tez/model/model.py\", line 324, in fit\n","    train_loss = self.train_one_epoch(self.train_loader)\n","  File \"/usr/local/lib/python3.6/dist-packages/tez/model/model.py\", line 193, in train_one_epoch\n","    for b_idx, data in enumerate(tk0):\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1104, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1085, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1111, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/_utils.py\", line 428, in reraise\n","    raise self.exc_type(msg)\n","FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"model_transfer_v3.py\", line 112, in __getitem__\n","    img = np.load(os.path.join(self.image_dir, filename)).astype(np.float32)\n","  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 416, in load\n","    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n","FileNotFoundError: [Errno 2] No such file or directory: 'data/fashion-dataset/big_model1_vectors/9655.npy'\n","\n","  0% 0/1098 [00:01<?, ?it/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k5-KW1t3ubmP"},"source":["## Stage 4 : train from efficient vectors"]},{"cell_type":"code","metadata":{"id":"hvlBKFOCRRvj"},"source":["# !ls ./data/fashion-dataset/efficientnet_vectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbZzehM7ufhX"},"source":["# !python model_transfer_v2.py --stage='train_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#               --intermediate_vector_path='./data/fashion-dataset/efficient_vectors' --model_name='fashion_efficient' \\\r\n","#               --input_vector_size=1536 --intermediate_vector_size=512  --epochs 1\r\n","\r\n","              \r\n","\r\n","\r\n","!python model_transfer_v3.py --stage='train_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","          --intermediate_vector_path='./data/fashion-dataset/efficientnet_vectors' --model_name='fashion_efficient' \\\r\n","          --input_vector_size=1536 --intermediate_vector_size=512 --root_df='df_final.csv' --train_df='train.csv' --val_df='val.csv' --batch_size=32 --epochs=20 --save_interval=5\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9dDDRHmumQj"},"source":["## Stage 5 : extract final vectors"]},{"cell_type":"code","metadata":{"id":"nP8OcPLRSA_Q"},"source":["# !ls ./models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sewJLrhZuqZA"},"source":["# !python model_transfer_v2.py --stage='extract_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#       --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --output_vector_path='./data/fashion-dataset/final_vectors' \\\r\n","#       --model_name='fashion_big_model1' --input_vector_size=512 --intermediate_vector_size=128  --epochs  1\r\n","\r\n","\r\n","#! nohup python model_transfer_v2.py --stage='extract_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#      --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --output_vector_path='./data/fashion-dataset/final_vectors' \\\r\n","#      --model_name='fashion_big_model1' --input_vector_size=512 --intermediate_vector_size=128  --epochs  1   &\r\n","\r\n","\r\n","            \r\n","!python model_transfer_v3.py --stage='extract_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","      --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors'  --output_vector_path='./data/fashion-dataset/final_vectors' \\\r\n","      --model_name='fashion_big_model1_best' --input_vector_size=512 --intermediate_vector_size=128 --root_df='df_final.csv' --test_df='df_final.csv'\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsU67wpKuyta"},"source":["## Stage 5 : train from logistic regression"]},{"cell_type":"code","metadata":{"id":"1nomot9Nu16Q"},"source":["# ! python model_transfer_v2.py --stage='check_vectors' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","#          --intermediate_vector_path='./data/fashion-dataset/final_vectors' \\\r\n","#          --model_name='logistic' --logistic_reg_targ_col='gender'\r\n","\r\n","\r\n","!python model_transfer_v3.py --stage='check_vectors' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","        --intermediate_vector_path='./data/fashion-dataset/final_vectors' --model_name='logistic' \\\r\n","        --logistic_reg_targ_col='gender' --root_df='df_final.csv' --train_df='train.csv' --val_df='val.csv' \r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlFZrR56HcVM"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"62U0rexW1Vto"},"source":["## Stage 6: Test :> Extract Big Model1"]},{"cell_type":"code","metadata":{"id":"mcC8xG621VAr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611292830794,"user_tz":-330,"elapsed":1566611,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"363e4ca1-5596-453c-f928-2d5a3b50c10a"},"source":["!python model_transfer_v3.py --stage='extract_bigmodel1' --base_path='./' --image_path='./data/test_dataset/images' \\\r\n","        --output_vector_path='./data/test_dataset/big_model1_vectors' --model_name='model' --root_df='df_final.csv' --test_df='test.csv' --batch_size=32"],"execution_count":null,"outputs":[{"output_type":"stream","text":["extract_bigmodel1\n","Total Number of Images:  6760\n","Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n","100% 212/212 [25:52<00:00,  7.32s/it]\n","Stage extract_bigmodel1 Finished ------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lmEIoWqUdV2H"},"source":["# while True:\r\n","#   pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Asu3HDuiHlE0"},"source":["## Stage 7: Test :> extract final vectors"]},{"cell_type":"code","metadata":{"id":"Bdzgxs0HhkFj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611294633292,"user_tz":-330,"elapsed":1511373,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"0ca6b8c4-04e0-499f-96f0-b25d879b7b03"},"source":["!python model_transfer_v3.py --stage='extract_small' --base_path='./' --image_path='./data/test_dataset/images' \\\r\n","          --intermediate_vector_path='./data/test_dataset/big_model1_vectors'  --output_vector_path='./data/test_dataset/final_vectors' \\\r\n","          --model_name='fashion_big_model1_best' --input_vector_size=512 --intermediate_vector_size=128 --root_df='df_final.csv' --test_df='test.csv' --batch_size=32"],"execution_count":null,"outputs":[{"output_type":"stream","text":["extract_small\n","Total Number of Images:  6760\n","Starting Extract Vector from Small model -------------------------\n","100% 212/212 [25:03<00:00,  7.09s/it]\n","Stage: extract_small Finished -------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"238wUS3sHnyc"},"source":["## Stage 8 : Test :> prediction"]},{"cell_type":"markdown","metadata":{"id":"ZSH3heVTcEzH"},"source":["**This will generate pred_df.csv and if label are present in input df then it will also calculate accuracy**"]},{"cell_type":"code","metadata":{"id":"288GmoHahkUq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611296273076,"user_tz":-330,"elapsed":32835,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"0c205cd6-5212-4b11-a426-81fcc340f33f"},"source":["# Prdiction on labeled data\r\n","!python model_transfer_v3.py --stage='predict_small' --base_path='./' --image_path='./data/fashion-dataset/images' \\\r\n","        --intermediate_vector_path='./data/fashion-dataset/big_model1_vectors' --model_name='fashion_big_model1_best' --input_vector_size=512 \\\r\n","        --intermediate_vector_size=128 --root_df='df_final.csv' --test_df='df_final_with_original_class.csv' --base_df='df_final_with_original_class.csv' \\\r\n","        --pred_df='df_final_pred.csv' --batch_size=32"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict_small\n","Total Number of Images:  29163\n","Starting Extract Vector from Small model -------------------------\n","100% 905/905 [00:23<00:00, 37.72it/s]\n","(28946, 8)\n","Class Name: gender ========== Accuracy: 0.917\n","Class Name: masterCategory ========== Accuracy: 0.995\n","Class Name: subCategory ========== Accuracy: 0.972\n","Class Name: articleType ========== Accuracy: 0.900\n","Class Name: baseColour ========== Accuracy: 0.709\n","Class Name: season ========== Accuracy: 0.708\n","Class Name: usage ========== Accuracy: 0.926\n","Prediction file saved to  df_final_pred.csv\n","Stage: predict_small Finished ------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lX31kKwEhkxb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611296236161,"user_tz":-330,"elapsed":14278,"user":{"displayName":"Subhan De","photoUrl":"","userId":"08832275770808375489"}},"outputId":"9667adf3-f483-498a-94c0-6cc13b20cfe4"},"source":["# Prdiction on unlabeled data\r\n","\r\n","!python model_transfer_v3.py --stage='predict_small' --base_path='./' --image_path='./data/test_dataset/images' \\\r\n","        --intermediate_vector_path='./data/test_dataset/big_model1_vectors' --model_name='fashion_big_model1_best' --input_vector_size=512 \\\r\n","        --intermediate_vector_size=128 --root_df='df_final.csv' --test_df='test.csv' --base_df='df_final_with_original_class.csv' \\\r\n","        --batch_size=32 --pred_df='test_pred.csv'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict_small\n","Total Number of Images:  6760\n","Starting Extract Vector from Small model -------------------------\n","100% 212/212 [00:05<00:00, 36.23it/s]\n","(6760, 8)\n","Prediction file saved to  test_pred.csv\n","Stage: predict_small Finished ------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PechVFT-hlBL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JljjczSUN8m4"},"source":["# Batch"]},{"cell_type":"code","metadata":{"id":"jwIlf2-lN7vF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611037583275,"user_tz":-540,"elapsed":38450,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"da9b34e3-a984-4fa1-b9fb-0e38be4ec131"},"source":["root =   \"/content/drive/My Drive/Colab Notebooks/shared_session/img/\"\r\n","\r\n","!pip install tez -y\r\n","!pip install efficientnet_pytorch\r\n","!pip uninstall albumentations\r\n","!pip install albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tez in /usr/local/lib/python3.6/dist-packages (0.0.8)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tez) (1.7.0+cu101)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (3.7.4.3)\n","Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.7.0+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n","Uninstalling albumentations-0.1.12:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/albumentations-0.1.12.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/albumentations/*\n","Proceed (y/n)? y\n","  Successfully uninstalled albumentations-0.1.12\n","Collecting albumentations\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 12.9MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.5)\n","Collecting opencv-python-headless>=4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n","\u001b[K     |████████████████████████████████| 37.6MB 82kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n","Installing collected packages: imgaug, opencv-python-headless, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LorWpE7uSSHi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mC2UscasSSaD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llimdnuCN7xv"},"source":["#### Extract from Big Model\r\n","\r\n","!python3 extract_vectors_big_model.py  --"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5bKDmryN70m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkY11o6AN73T"},"source":["#### Extract big model vector\r\n","!  python3 model_transfer.py  --stage extract_bigmodel1   --intermediate_vector_path  './data/fashion-dataset/bigmodel_vectors/' \r\n","\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0FV_KQRN76A","colab":{"base_uri":"https://localhost:8080/","height":640},"executionInfo":{"status":"ok","timestamp":1611038577385,"user_tz":-540,"elapsed":8716,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"9184238f-c650-43f8-dfcd-6f625bca5611"},"source":["#### Train Small model\r\n","\r\n","!  python3 model_transfer.py  --stage train_small   --intermediate_vector_path  './data/fashion-dataset/bigmodel_vectors/' \\\r\n","        --epochs 1  \r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\"\"\"\r\n","\r\n","!  nohup python3 model_transfer.py  --stage train_small   --intermediate_vector_path  './data/fashion-dataset/bigmodel_vectors/' \\\r\n","        --epochs 1   &\r\n","\"\"\"\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_small\n","Starting  Train Small model -------------------------\n","  0% 0/2895 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"model_transfer.py\", line 360, in <module>\n","    fp16          = args.fp16,\n","  File \"/usr/local/lib/python3.6/dist-packages/tez/model/model.py\", line 297, in fit\n","    train_loss = self.train_one_epoch(self.train_loader, device)\n","  File \"/usr/local/lib/python3.6/dist-packages/tez/model/model.py\", line 178, in train_one_epoch\n","    for b_idx, data in enumerate(tk0):\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1104, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1085, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1111, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/_utils.py\", line 428, in reraise\n","    raise self.exc_type(msg)\n","FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"model_transfer.py\", line 99, in __getitem__\n","    img = np.load(os.path.join(self.image_dir, filename))\n","  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 416, in load\n","    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n","FileNotFoundError: [Errno 2] No such file or directory: 'data/fashion-dataset/bigmodel_vectors/14044.npy'\n","\n","  0% 0/2895 [00:01<?, ?it/s]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n\\n!  nohup python3 model_transfer.py  --stage train_small   --intermediate_vector_path  './data/fashion-dataset/bigmodel_vectors/'         --epochs 1   &\\n\""]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"NdHhl-D1N78r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611038825492,"user_tz":-540,"elapsed":898,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"68018c58-d199-4e42-fcc0-af2b27e48aaf"},"source":["# ! ls   './data/fashion-dataset/bigmodel_vectors/'    | wc -l\r\n","\r\n","\r\n","! ls   ./data/fashion-dataset/bigmodel_vectors/*1404*  \r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["./data/fashion-dataset/bigmodel_vectors/14040.npy\n","./data/fashion-dataset/bigmodel_vectors/14041.npy\n","./data/fashion-dataset/bigmodel_vectors/14047.npy\n","./data/fashion-dataset/bigmodel_vectors/14048.npy\n","./data/fashion-dataset/bigmodel_vectors/14049.npy\n","./data/fashion-dataset/bigmodel_vectors/41404.npy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-nAipkAKN7_P"},"source":["#### Extract from Small Model\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2Udr-qnN8CG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611037596289,"user_tz":-540,"elapsed":560,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"d93059a2-a7b9-417f-f3cf-d72fc4a1ad51"},"source":["! pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/shared_session/img\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G2w-WW_-N8Em","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611038156718,"user_tz":-540,"elapsed":2495,"user":{"displayName":"N K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8t5mrIeOihMRbpM9-RqqZ-39kIBr4Zas4AyQnrkE=s64","userId":"04058280849795708816"}},"outputId":"af6b435c-6c5b-42ab-937a-211125a63f15"},"source":["#### Test from small model vectors and output accuracy\r\n","\r\n","!  python3 model_transfer.py  --stage check_vectors   --intermediate_vector_path  './data/fashion-dataset/final_vectors_v01/'\r\n","\r\n","\r\n","\r\n","\r\n","# !  nohup python3 model_transfer.py  --stage check_vectors   --imtermediate_vector_path  './data/fashion-dataset/final_vectors_v01/' \\\r\n","#          &\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["check_vectors\n","Starting   ------------------------- check_vectors\n","Traceback (most recent call last):\n","  File \"model_transfer.py\", line 397, in <module>\n","    train_dataset = FashionImageDataset(IMAGE_PATH=INTERMEDIATE_VECTOR_PATH, DF_PATH=BASE_DIR/'train.csv')\n","  File \"model_transfer.py\", line 57, in __init__\n","    self.df              = pd.read_csv(DF_PATH)\n","  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 688, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 454, in _read\n","    parser = TextFileReader(fp_or_buf, **kwds)\n","  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 948, in __init__\n","    self._make_engine(self.engine)\n","  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1180, in _make_engine\n","    self._engine = CParserWrapper(self.f, **self.options)\n","  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 2010, in __init__\n","    self._reader = parsers.TextReader(src, **kwds)\n","  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n","  File \"pandas/_libs/parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\n","FileNotFoundError: [Errno 2] No such file or directory: 'train.csv'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uCkS7YQzN8Hd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kg9OvkScN8Ka"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bid67KDN8M1"},"source":[""],"execution_count":null,"outputs":[]}]}